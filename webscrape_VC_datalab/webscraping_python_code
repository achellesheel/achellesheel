import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

# Base URL of the website to scrape
base_url = "https://kando.tech/investors/all"

# Initialize a list to store all scraped data
all_data = []

# Column names
columns = ["  ","Investor", "Country", "Profile", "Deals"]

# Set to track visited pages and avoid duplicates
visited_pages = set()

# List to store problematic pages
problematic_pages = []

# Function to scrape a single page
def scrape_page(url, page_number):
    try:
        # Send a GET request to the website
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # Raise an exception for HTTP errors (4xx or 5xx)

        # Parse the HTML content using BeautifulSoup
        soup = BeautifulSoup(response.content, 'html.parser')

        # Find the table or container holding the data
        table = soup.find('table')  # Adjust based on the website's structure
        if table:
            rows = table.find_all('tr')  # Adjust based on the website's structure

            # Loop through the rows and extract data
            for row in rows:
                columns_data = row.find_all('td')  # Adjust based on the website's structure
                if len(columns_data) == 5:  # Ensure there are 4 columns (Investor, Country, Profile, Deals)
                    row_data = [col.text.strip() for col in columns_data]
                    all_data.append(row_data)
        else:
            print(f"No table found on page {page_number}.")

    except requests.exceptions.RequestException as e:
        print(f"Error scraping page {page_number}: {e}")
        problematic_pages.append(page_number)

# Start scraping from page 0
current_page = 0
while True:
    if current_page == 0:
        url = base_url  # First page has no ?page parameter
    else:
        url = base_url + f"?page={current_page}"  # Subsequent pages

    # Skip if the page has already been visited
    if url in visited_pages:
        print(f"Page {current_page} is a duplicate. Skipping...")
        current_page += 1
        continue

    print(f"Scraping page {current_page}...")
    scrape_page(url, current_page)
    visited_pages.add(url)  # Mark the page as visited

    # Check if there are more pages
    # (You can add logic here to detect if the "Next" button is disabled or if no data is found)
    # For now, we assume there are 634 pages based on your input
    if current_page >= 634:
        break

    current_page += 1

    # Add a delay to avoid overloading the server
    time.sleep(2)

# Convert the data into a pandas DataFrame
df = pd.DataFrame(all_data, columns=columns)

# Save the DataFrame to an Excel file
df.to_excel("investors_data_all_pages.xlsx", index=False)
print("Data from all pages saved to investors_data_all_pages.xlsx")

# Log problematic pages
if problematic_pages:
    print(f"The following pages were problematic and skipped: {problematic_pages}")
    with open("problematic_pages.txt", "w") as file:
        file.write("\n".join(map(str, problematic_pages)))
    print("Problematic pages logged in problematic_pages.txt")
